{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 17:25:22.103061: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 17:25:22.836906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/disi/ml/mlvenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.download_data import *\n",
    "from utils.epochs_loop import *\n",
    "from utils.data_aug_utils import *\n",
    "from utils.read_dataset import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Aggiungere nella pipeline di preparazione dei dati di pytorch la possibilità di fare da augmentation.\n",
    "- I logs sono salvati in datasets al momento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook usage\n",
    "This notebook is used to streamline as much as possible the data setup for the project. We have trained models on data downloaded from the internet \n",
    "as well as present in the Torch and timm libraries. \n",
    "\n",
    "This notebook will help you set up the data in the correct directory structure.\n",
    "It will be used to download the data, extract it, and set it up in the correct directory structure.\n",
    "If you want to work using datasets preloaded in Torch or timm, you can skip this notebook.\n",
    "\n",
    "Everything that needs to be edited is marked with \"@edit\". It is procedural, therefore do not skip steps and go one cell at the time. \n",
    "\n",
    "At the moment less common cases such as image folder + .mat file or .csv file are not yet implemented in this notebook (csv function exists) or not implemented at all (.mat function doesn't exist at all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/prasanshasatpathy/soil-types\n",
      "Kaggle Dataset downloaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually edit the three following lines, save the dataset wherever you wish, but the \"datasets\" folder is recommended\n",
    "# make sure to use the absolute path, not relative\n",
    "# @edit\n",
    "dataset_url = \"https://www.kaggle.com/datasets/prasanshasatpathy/soil-types\"\n",
    "# @edit\n",
    "folder_target = \"/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets\"\n",
    "download_dataset_kaggle(dataset_url, folder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset from a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m i \u001b[38;5;241m=\u001b[39m download_dataset(dataset_url, folder_target)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Manually move it in the datasets folder if you do not need to unzip it, otherwhise extracting it with the following function\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# will do it for you\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# @edit\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mextract_tgz\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/qui/intromlproject_su cui lavorare/utils/download_data.py:204\u001b[0m, in \u001b[0;36mextract_tgz\u001b[0;34m(tgz_path, extract_to)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Open the tgz file and extract it\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(tgz_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m--> 204\u001b[0m     \u001b[43mtar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles have been extracted to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_to\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:2257\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2253\u001b[0m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[1;32m   2254\u001b[0m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[1;32m   2256\u001b[0m         directories\u001b[38;5;241m.\u001b[39mappend(tarinfo)\n\u001b[0;32m-> 2257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:2320\u001b[0m, in \u001b[0;36mTarFile._extract_one\u001b[0;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_fatal_error(e)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:2420\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchown(tarinfo, targetpath, numeric_owner)\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39missym():\n\u001b[0;32m-> 2420\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutime(tarinfo, targetpath)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tarfile.py:2556\u001b[0m, in \u001b[0;36mTarFile.chmod\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2556\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExtractError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not change mode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "dataset_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "folder_target = \"/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets\"\n",
    "i = download_dataset(dataset_url, folder_target)\n",
    "# Manually move it in the datasets folder if you do not need to unzip it, otherwhise extracting it with the following function\n",
    "# will do it for you\n",
    "# @edit\n",
    "extract_tgz(\"/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets.tar.gz\", folder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train_val_test folders successfully\n",
      "Deleting folder: /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Soil types/Yellow Soil\n",
      "Deleting folder: /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Soil types/Peat Soil\n",
      "Deleting folder: /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Soil types/Black Soil\n",
      "Deleting folder: /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Soil types/Laterite Soil\n",
      "Deleting folder: /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Soil types/Cinder Soil\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "# make sure to add a / in front of the folder name \n",
    "dataset_folder = folder_target + \"/Soil types\"\n",
    "image_generator =  setup_data_generator(rotation_range=40,\n",
    "                                        width_shift_range=0.2, \n",
    "                                        height_shift_range=0.2,\n",
    "                                        shear_range=0.2, \n",
    "                                        zoom_range=0.2, \n",
    "                                        horizontal_flip=True, \n",
    "                                        fill_mode='nearest')\n",
    "\n",
    "create_train_val_test_folders(dataset_folder, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "cleanup(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 108 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "directory_path  = dataset_folder + \"/train\"\n",
    "train_generator = load_data_from_directory(directory_path, target_size=(300,300), batch_size=32)\n",
    "n_of_batches = 50\n",
    "\n",
    "#TODO write how the number of batches infuences the actual amount of images generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(train_generator):\n",
    "    if i >= n_of_batches:  # Stop after saving images from 50 batches\n",
    "        break\n",
    "    save_augmented_images(images, labels, directory_path, train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Val-Split\n",
    "If you augmented the images you are good to go, the dataset is ready to be used in the main function. Otherwhise edit and execute the following block of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "# make sure to add a / in front of the folder name \n",
    "dataset_folder = folder_target + \"/jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create train_val_test folders successfully\n"
     ]
    }
   ],
   "source": [
    "create_train_val_test_folders(dataset_folder, train_size=0.7, val_size=0.15, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets from Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGVCAircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz to /home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2753340328/2753340328 [00:35<00:00, 77742999.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b.tar.gz to /home/disi/ml/intromlproject/datasets\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "root = '/home/disi/ml/intromlproject/datasets'\n",
    "dataset = torchvision.datasets.FGVCAircraft(root=root, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "dataset_path = '/home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b/data/images'\n",
    "new_dataset_path = '/home/disi/ml/intromlproject/datasets'\n",
    "train_txt_path = '/home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b/data/images_family_train.txt'\n",
    "val_txt_path = '/home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b/data/images_family_train.txt'\n",
    "test_txt_path = '/home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b/data/images_family_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorganize_dataset_txt(dataset_path, train_txt_path, new_dataset_path, 'train')\n",
    "reorganize_dataset_txt(dataset_path, val_txt_path, new_dataset_path, 'val')\n",
    "reorganize_dataset_txt(dataset_path, test_txt_path, new_dataset_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped folder: /home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b.tar.gz\n",
      "Skipped folder: /home/disi/ml/intromlproject/datasets/val\n",
      "Skipped folder: /home/disi/ml/intromlproject/datasets/train\n",
      "Deleted folder: /home/disi/ml/intromlproject/datasets/fgvc-aircraft-2013b\n",
      "Skipped folder: /home/disi/ml/intromlproject/datasets/.txt\n",
      "Skipped folder: /home/disi/ml/intromlproject/datasets/test\n",
      "Skipped folder: /home/disi/ml/intromlproject/datasets/datasets.txt\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "cleanup('/home/disi/ml/intromlproject/datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowers102 / Oxford Flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102/102flowers.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344862509/344862509 [01:40<00:00, 3442214.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102/102flowers.tgz to /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102\n",
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102/imagelabels.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [00:00<00:00, 1198372.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102/setid.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14989/14989 [00:00<00:00, 17941901.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "\n",
    "root = '/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets'\n",
    "dataset = torchvision.datasets.Flowers102(root=root, download = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your original dataset, .mat file, and new dataset location\n",
    "# @edit\n",
    "dataset_path = '/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102/jpg'\n",
    "mat_path = '/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102/imagelabels.mat'\n",
    "new_dataset_path = '/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for each of your train, validation, and test sets\n",
    "reorganize_dataset_mat(dataset_path, mat_path, new_dataset_path, 'train')\n",
    "reorganize_dataset_mat(dataset_path, mat_path, new_dataset_path, 'val')\n",
    "reorganize_dataset_mat(dataset_path, mat_path, new_dataset_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting folder: /home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/flowers-102\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "cleanup('/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last step\n",
    "Once you reach the train-test-val configuration, run this final function to create a folder named as you wish that will contain train-test-val. This will be our root in the main.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "location = '/home/disi/ml/intromlproject/datasets'\n",
    "newfolder_name = 'Aerei'\n",
    "train_folder = '/home/disi/ml/intromlproject/datasets/train/'\n",
    "val_folder = '/home/disi/ml/intromlproject/datasets/val/'\n",
    "test_folder = '/home/disi/ml/intromlproject/datasets/test/'\n",
    "final_structure(location, newfolder_name, train_folder, val_folder, test_folder)"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Flowers102'\n",
    "path = os.path.dirname(img_root) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Flowers102\n"
     ]
    }
   ],
   "source": [
    "print(img_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/\n"
     ]
    }
   ],
   "source": [
    "folder = os.path.basename(img_root)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /home/disi/ml/intromlproject/datasets\n",
      "Folder: Aerei\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the root path\n",
    "img_root = '/home/disi/ml/intromlproject/datasets/Aerei'\n",
    "\n",
    "# Standardize the path\n",
    "standardized_path = os.path.normpath(img_root)\n",
    "\n",
    "# Split the path into directory and the last component\n",
    "path, folder = os.path.split(standardized_path)\n",
    "\n",
    "# Print the results\n",
    "print(\"Path:\", path)\n",
    "print(\"Folder:\", folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
